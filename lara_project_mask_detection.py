# -*- coding: utf-8 -*-
"""Lara Project -  Mask Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tGt3qg0w_whq00HMe1HqLaEKpJgsiFol

##Mask Detection

Â© 2022 Zaka AI, Inc. All Rights Reserved.

---
The goal of this project is to build a model that can detect whether a person is wearing a mask or not. You would be downloading the data from kaggle and then building you model while we guide you through the steps.

##Getting the Data

We start by getting the data. The data that we want is on kaggle and you can access it through this link: https://www.kaggle.com/omkargurav/face-mask-dataset What you will have to do, is to search for a way that allows you to download the dataset from kaggle directly into google colab (or your google drive). This process would save you the trouble from downloading the dataset locally and then uploading it to use it in colab.
"""

#!pip install opendatasets

import opendatasets as od

"""The following cell uses kaggle's API to download the dataset"""

od.download("https://www.kaggle.com/omkargurav/face-mask-dataset")

#Do your Research
#Test Your Zaka
!pip install kaggle
# from google.colab import files

# uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))
  
# #Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions list

!kaggle datasets download -d omkargurav/face-mask-dataset

"""Once the dataset is downloaded, it is going to be zipped, and in order to use it, you need to unzip it. Here you have the option of unzipping it in the environment or in your google drive."""

#Test Your Zaka
!unzip face-mask-dataset.zip

"""##Importing the Libraries

Now, it is time to import the libraries that we need.
"""

import os
import shutil
import random
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
import cv2

"""If we look at the data that we extracted, we can see that it is located in 2 folders: with_mask and without_mask. What we need to do now, is to create a hierarchy of folders that would help us specify the training, validation, and testing data. In order to do this you have to use the **os** and the **shutil** libraries that would help you creating folders, and moving images from a folder to another. In simple terms, the libraries helps you perform the same things that you do on your computer with your operating system, but in a programmatic way.

The initial dataset contains a lot of images belonging to the 2 classes, what we will do is randomly sample from this dataset so that we reduce the training and the tuning time.

We need to have in our final directories the following:
* Training: 500 images "with_mask", and 500 images "without_mask"
* Validation: 100 images "with_mask", and 100 images "without_mask"
* Testing: 50 images "with_mask", and 50 images "without_mask"
"""

#Test Your Zaka --- Create the Train, Validation and Testing Folder
cwd = os.getcwd()
print(cwd)
directory=cwd +'/dataset'
os.mkdir(directory)

#train folder
train_set= directory +'/train'
os.mkdir(train_set)
os.path.join(directory,train_set) #!
#print(len(train_set))
#validation folder
validation_set = directory +'/validation'
os.mkdir(validation_set)

#test folder
test_set = directory +'/test'
os.mkdir(test_set)

# img_path = "/content/dataset/train/with_mask/with_mask_1.jpg"
# img = load_img(img_path)
# print(type(img))
# print(img.format)
# print(img.mode)
# print(img.size) #pixels size so it is 525*350

#Test Your Zaka --- Create the classes folders inside of each folder you created above
os.makedirs(train_set +"/with_mask")
os.makedirs(test_set +"/with_mask")
os.makedirs(validation_set +"/with_mask")

os.makedirs(train_set +"/without_mask")
os.makedirs(test_set +"/without_mask")
os.makedirs(validation_set +"/without_mask")

source1 = '/content/data/with_mask'
# with_mask1 = shutil.copytree(source1, '/content/dataset/train/with_mask')
# with_mask2 = shutil.copytree(source1, '/content/dataset/validation/with_mask')
# with_mask3 = shutil.copytree(source1, '/content/dataset/test/with_mask')
# with_mask= os.path.join(train_set,with_mask1)
source2 = '/content/data/without_mask'
# without_mask1 = shutil.copytree(source2, '/content/dataset/train/without_mask')
# without_mask2 = shutil.copytree(source2, '/content/dataset/validation/without_mask')
# without_mask3 = shutil.copytree(source2, '/content/dataset/test/without_mask')

"""
## Opening the directory exported that contains data with mask

**Splitting the entire data set into three sets**
*   Training set of 500 images
*   Validation set of 100 images 
*   Test set of 50 images
"""

Data_with_mask = '/content/face-mask-dataset/data/with_mask'

all_data_mask = os.listdir(Data_with_mask)


mask_training_set = all_data_mask[0:500]

mask_validation_set =  all_data_mask[500:600]

mask_test_set = all_data_mask[600:650]




print(f'The entire dataset for mask: {len(all_data_mask)}')

print(f'The training set for mask: {len(mask_training_set)}')

print(f'The validation set for mask: {len(mask_validation_set)}')

print(f'The test set for mask: {len(mask_test_set)}')

for filename in mask_training_set:
  dst_filename =  r"/content/dataset/train/with_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/with_mask/" + filename
  shutil.move(src_filename, dst_filename)


for filename in mask_validation_set:
  dst_filename =  r"/content/dataset/validation/with_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/with_mask/" + filename
  shutil.move(src_filename, dst_filename)



for filename in mask_test_set:
  dst_filename =  r"/content/dataset/test/with_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/with_mask/" + filename
  shutil.move(src_filename, dst_filename)

"""# Opening the directory exported that contains data without mask

**Splitting the entire data set into three sets**
*   Training set of 500 images
*   Validation set of 100 images 
*   Test set of 50 images


"""

Data_without_mask = '/content/face-mask-dataset/data/without_mask'

all_data_without_mask = os.listdir(Data_without_mask)


without_mask_training_set = all_data_without_mask[0:500]

without_mask_validation_set =  all_data_without_mask[500:600]

without_mask_test_set = all_data_without_mask[600:650]



print(f'The entire dataset for without mask: {len(all_data_without_mask)}')

print(f'The training set for without mask: {len(without_mask_training_set)}')

print(f'The validation set for without mask: {len(without_mask_validation_set)}')

print(f'The test set for without mask: {len(without_mask_test_set)}')

for filename in without_mask_training_set:
  dst_filename =  r"/content/dataset/train/without_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/without_mask/" + filename
  shutil.move(src_filename, dst_filename)


for filename in without_mask_validation_set:
  dst_filename =  r"/content/dataset/validation/without_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/without_mask/" + filename
  shutil.move(src_filename, dst_filename)



for filename in without_mask_test_set:
  dst_filename =  r"/content/dataset/test/without_mask/" + filename
  src_filename = r"/content/face-mask-dataset/data/without_mask/" + filename
  shutil.move(src_filename, dst_filename)

"""##Inspecting the Dataset

Now we will see some characteristics of our dataset.

Define 3 variables: **training_path**, **validation_path**, and **testing_path** so that you can use them for the rest of the colab.
"""

#Test Your Zaka
training_path = '/content/dataset/train'
validation_path = '/content/dataset/validation'
testing_path = '/content/dataset/test'

with_mask_training_path= training_path + '/with_mask'
without_mask_trainning_path = training_path + '/without_mask'
training_with_path = os.path.join(with_mask_training_path,without_mask_trainning_path)

with_mask_validation_path= validation_path + '/with_mask'
without_mask_validation_path = validation_path + '/without_mask'
validation_with_path = os.path.join(with_mask_validation_path,without_mask_validation_path)

with_mask_testing_path= testing_path + '/with_mask'
without_mask_testing_path = testing_path + '/without_mask'
testing_with_path = os.path.join(with_mask_testing_path,without_mask_testing_path)

print(testing_with_path)

training_with_mask = len(os.listdir(training_path + '/with_mask'))
training_without_mask = len(os.listdir(training_path + '/without_mask'))

validation_with_mask = len(os.listdir(validation_path + '/with_mask'))
validation_without_mask = len(os.listdir(validation_path + '/without_mask'))

testing_with_mask = len(os.listdir(testing_path + '/with_mask'))
testing_without_mask = len(os.listdir(testing_path + '/without_mask'))

"""To make sure that everythinh went correctly, write a code that counts the number of images that you have in your training directory for each of the 2 categories: with_mask and without_mask"""

#Test Your Zaka
with_mask = 0
without_mask = 0
# Iterate directory
for path in os.listdir(training_path+ '/with_mask'):
        with_mask += 1
for path in os.listdir(training_path+ '/without_mask'):
        without_mask +=1
      
print('File count:', with_mask)
print('File count:', without_mask)

#or
# print(training_with_mask)
# print(training_without_mask)

"""Do the same for the validation and the testing folders"""

#Test Your Zaka
# with_mask = 0
# without_mask = 0
# # Iterate directory
# for path in os.listdir(validation_path+ '/with_mask'):
#         with_mask += 1
# for path in os.listdir(validation_path+ '/without_mask'):
#         without_mask +=1
      
# print('File count:', with_mask)
# print('File count:', without_mask)

print(validation_with_mask)
print(validation_without_mask)

#Test Your Zaka
with_mask = 0
without_mask = 0
# Iterate directory
for path in os.listdir(testing_path+ '/with_mask'):
        with_mask += 1
for path in os.listdir(testing_path+ '/without_mask'):
        without_mask +=1
      
print('File count:', with_mask)
print('File count:', without_mask)

#another way
with_mask = 0
for root_dir, cur_dir, files in os.walk(testing_path+ '/with_mask'):
    with_mask += len(files)
print('file count:', with_mask)

without_mask = 0
for root_dir, cur_dir, files in os.walk(testing_path+ '/without_mask'):
    without_mask += len(files)
print('file count:', without_mask)

"""Write a code that shows 5 random images for people with mask from your training set. """

from traitlets.config.loader import FileConfigLoader
from google.colab.patches import cv2_imshow
from PIL import Image
import random
#Test Your Zaka
#dirs = os.listdir(training_path+'/with_mask')
random_list = []
image_random_number = 5
for folder in os.listdir(training_path+ '/with_mask'):
    random_list.append(folder)
    
# now pick 5 random indices 
rand_numbers = random.sample(range(1, 500), 5)
# print(rand_numbers)

img_1 = training_path + '/with_mask/' + random_list[0]
img_2 = training_path + '/with_mask/' + random_list[1]
img_3 = training_path + '/with_mask/' + random_list[2]
img_4 = training_path + '/with_mask/' + random_list[3]
img_5 = training_path + '/with_mask/' + random_list[4]


rows = 2
columns = 3
fig = plt.figure(figsize=(10, 7))

image1 = cv2.imread(img_1, cv2.IMREAD_COLOR)

image2 = cv2.imread(img_2, cv2.IMREAD_COLOR)

image3 = cv2.imread(img_3, cv2.IMREAD_COLOR)

image4 = cv2.imread(img_4, cv2.IMREAD_COLOR)

image5 = cv2.imread(img_5, cv2.IMREAD_COLOR)


fig.add_subplot(rows, columns, 1)
plt.imshow(image1)

fig.add_subplot(rows, columns, 2)
plt.imshow(image2)


fig.add_subplot(rows, columns, 3)
plt.imshow(image3)


fig.add_subplot(rows, columns, 4)
plt.imshow(image4)


fig.add_subplot(rows, columns, 5)
plt.imshow(image5)

"""Do the same for people without mask."""

#Test Your Zaka
#dirs = os.listdir(training_path+'/with_mask')
random_list = []
image_random_number = 5
for folder in os.listdir(training_path+ '/without_mask'):
    random_list.append(folder)

# now pick 5 random indices 
rand_numbers = random.sample(range(1, 500), 5)
# print(rand_numbers)

img_1 = training_path + '/without_mask/' + random_list[0]
img_2 = training_path + '/without_mask/' + random_list[1]
img_3 = training_path + '/without_mask/' + random_list[2]
img_4 = training_path + '/without_mask/' + random_list[3]
img_5 = training_path + '/without_mask/' + random_list[4]


rows = 2
columns = 3
fig = plt.figure(figsize=(10, 7))

image1 = cv2.imread(img_1, cv2.IMREAD_COLOR)

image2 = cv2.imread(img_2, cv2.IMREAD_COLOR)

image3 = cv2.imread(img_3, cv2.IMREAD_COLOR)

image4 = cv2.imread(img_4, cv2.IMREAD_COLOR)

image5 = cv2.imread(img_5, cv2.IMREAD_COLOR)


fig.add_subplot(rows, columns, 1)
plt.imshow(image1)

fig.add_subplot(rows, columns, 2)
plt.imshow(image2)


fig.add_subplot(rows, columns, 3)
plt.imshow(image3)


fig.add_subplot(rows, columns, 4)
plt.imshow(image4)


fig.add_subplot(rows, columns, 5)
plt.imshow(image5)

# from tensorflow.keras.preprocessing.image import load_img 
# from tensorflow.keras.preprocessing.image import img_to_array 
# from tensorflow.keras.preprocessing.image import array_to_img

# # load the image
# #img_path = "computer-vision-course/deep_learning/data/image2.jpg"
# without_mask_path = training_path + '/without_mask'

# for i in os.listdir(without_mask_path) :
#   full_img_path = without_mask_path + '/' + i
#   img = load_img(full_img_path)
#   print(type(img))

# # convert to numpy array
#   img_array2 = img_to_array(img) 
#   print(img_array2.dtype)
#   print(img_array2.shape)

# # convert back to image
#   img_pil = array_to_img(img_array2)
#   print(type(img))

# from tensorflow.keras.preprocessing.image import load_img 
# from tensorflow.keras.preprocessing.image import img_to_array 
# from tensorflow.keras.preprocessing.image import array_to_img

# # load the image
# #img_path = "computer-vision-course/deep_learning/data/image2.jpg"
# #training_with_path = os.path.join(with_mask_path,without_mask_path)

# for i in os.listdir(training_with_path) :
#   full_img_path = training_with_path + '/' + i
#   img = load_img(full_img_path)
#   print(type(img))

# # convert to numpy array
#   img_array1 = img_to_array(img) 
#   print(img_array1.dtype)
#   print(img_array1.shape)

#   samples1 = tf.expand_dims(img_array1, 0)
#   print(samples1.shape)
# # convert back to image
#   img_pil = array_to_img(img_array1)
#   print(type(img))

# from tensorflow.keras.preprocessing.image import load_img 
# from tensorflow.keras.preprocessing.image import img_to_array 
# from tensorflow.keras.preprocessing.image import array_to_img

# # load the image
# #img_path = "computer-vision-course/deep_learning/data/image2.jpg"
# #training_with_path = os.path.join(with_mask_path,without_mask_path)

# for i in os.listdir(validation_with_path) :
#   full_img_path = validation_with_path + '/' + i
#   img = load_img(full_img_path)
#   print(type(img))

# # convert to numpy array
#   img_array2 = img_to_array(img) 
#   print(img_array2.dtype)
#   print(img_array2.shape)

#   samples2 = tf.expand_dims(img_array2, 0)
#   print(samples2.shape)

# # convert back to image
#   img_pil = array_to_img(img_array2)
#   print(type(img))

# from tensorflow.keras.preprocessing.image import load_img 
# from tensorflow.keras.preprocessing.image import img_to_array 
# from tensorflow.keras.preprocessing.image import array_to_img

# # load the image
# #img_path = "computer-vision-course/deep_learning/data/image2.jpg"
# #training_with_path = os.path.join(with_mask_path,without_mask_path)

# for i in os.listdir(validation_with_path) :
#   full_img_path = validation_with_path + '/' + i
#   img = load_img(full_img_path)
#   print(type(img))

# # convert to numpy array
#   img_array3 = img_to_array(img) 
#   print(img_array3.dtype)
#   print(img_array3.shape)

#   samples = tf.expand_dims(img_array3, 0)
#   print(samples.shape)

# # convert back to image
#   img_pil = array_to_img(img_array3)
#   print(type(img))

"""##Modeling

Define a model structure that can deal with the images that we have to classify them between the 2 classes.
"""

#Test Your Zaka
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, Flatten, Dropout

datagen = ImageDataGenerator(rescale=1.00/255.00)
train_it=datagen.flow_from_directory(training_path,batch_size=50,target_size=(256,256),class_mode='binary')
eval_it=datagen.flow_from_directory(validation_path,batch_size=50,target_size=(256,256),class_mode='binary')
test_it=datagen.flow_from_directory(testing_path,batch_size=50,target_size=(256,256),class_mode='binary',shuffle=False)


model=Sequential()
model.add(Conv2D(25,(3,3),input_shape=(256,256,3),activation='relu',strides=2))
#model.add(AveragePooling2D())
#model.add(Conv2D(20,(3,3),activation='relu'))
#model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(50,input_shape=(256,256,3), activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(25,input_shape=(256,256,3), activation='relu'))
model.add(Dense(1, activation='sigmoid'))



model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy','Recall','Precision'])
model.summary()

"""Train the model that you defined on the training data and evaluate it on the validation data. Feel free to tune the hyperparameters of your model until you reach a satisfying result on the validation set. </br>
**N.B:** Make sure to save the model training history in a variable to plot later the learning curve.
"""

#Test Your Zaka
history = model.fit(train_it, validation_data=eval_it, epochs=50, steps_per_epoch=10)

"""Plot the accuracy curve and see if your model is overfit."""

#Test Your Zaka
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for training and validation loss
plt.plot(history.history['recall'])
plt.plot(history.history['val_recall'])
plt.title('model recall')
plt.ylabel('recall')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for training and validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""[Share your Zaka]

##Evaluate the model

Now you will evaluate the model that you built on the testing set that you kept aside since the beginning.
"""

#Test Your Zaka
model.evaluate(test_it)

pred=model.predict(test_it)
y_pred = (pred > 0.5).astype(int)
print(y_pred)

y_true = test_it.labels
print(y_true)

"""Now we want to visualize the confusion matrix in order to see how much our classifier is good in predicting different classes."""

#Test Your Zaka
from sklearn.metrics import confusion_matrix, accuracy_score
import seaborn as sns
print("True predictions :" , accuracy_score(y_true, y_pred, normalize=False))
print("")
plt.figure(figsize = (10,5))
cm = confusion_matrix(y_true, y_pred)/np.sum(confusion_matrix(y_true, y_pred))
sns.heatmap(cm, annot=True, fmt= '.1%', lw=0.1, cmap='Blues', cbar=False)

"""##Error Analysis

Now we need to see for each of the images that we have in our testing set, what did the model predict. This helps us analyze the errors and try to think why the model predicted some things in the wrong way.
"""

#Test Your Zaka
acc = 0
for folder in os.listdir(testing_path):
  for img in os.listdir(testing_path + "/" + folder):
    print("image name:",img)
    image = cv2.imread(testing_path + "/" + folder + "/" +img)
    cv2_imshow(image)
    print("True image:",y_true[acc])

    print("Predicted image:",y_pred[acc].astype(int))
    acc = acc + 1